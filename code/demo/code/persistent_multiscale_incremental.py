# persistent_multiscale_incremental.py
# å®Œæ•´ç‰ˆï¼šå¸¦â€œç¬¬ä¸€æ¬¡å®Œæ•´è®­ç»ƒ + åç»­å°æ—¶å¢é‡è®­ç»ƒâ€çš„ MultiScaleModelManager
# ä»¥åŠä¸€ä¸ªå¯åœ¨ IDE é‡Œç›´æ¥è¿è¡Œçš„ main() æ¼”ç¤ºï¼ˆæ»šåŠ¨ k å°æ—¶ï¼‰

from __future__ import annotations
import json
from pathlib import Path
from dataclasses import dataclass
from typing import Optional, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler


# ============== åŸºç¡€æ¨¡å‹ï¼ˆä¸æ‚¨åŸç‰ˆä¸€è‡´ï¼Œä»…ä¿®æ­£ Transformer çš„ batch_first é€»è¾‘ï¼‰ ==============
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class MultiScaleModel(nn.Module):
    """
    LSTM(1d/1w) + Transformer(1m) â†’ ç‰¹å¾èåˆ â†’ ä¸ 1h æ‹¼æ¥è¿› GRU â†’ FC
    æ³¨æ„ï¼šTransformerä½¿ç”¨ batch_first=Trueï¼Œå› æ­¤ä¸éœ€è¦ permuteã€‚
    """
    def __init__(self, hidden_size: int):
        super().__init__()
        self.hidden_size = hidden_size

        # LSTMs (daily & weekly)
        self.lstm_1d = nn.LSTM(1, hidden_size, batch_first=True)
        self.lstm_1w = nn.LSTM(1, hidden_size, batch_first=True)

        # Transformer (monthly)
        self.input_projection = nn.Linear(1, hidden_size)
        self.transformer_1m = nn.Transformer(
            d_model=hidden_size, nhead=4, num_encoder_layers=2, batch_first=True
        )

        # Feature fusion
        self.feature_fusion = nn.Linear(hidden_size * 3, hidden_size)

        # GRU on fused features + raw 1h
        self.gru = nn.GRU(hidden_size + 1, hidden_size, batch_first=True)

        # Final regressor
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        x_1h, x_1d, x_1w, x_1m = x["1h"], x["1d"], x["1w"], x["1m"]  # [B, T, 1]

        # LSTMs
        _, (h_1d, _) = self.lstm_1d(x_1d)
        h_1d = h_1d[-1]  # [B, H]

        _, (h_1w, _) = self.lstm_1w(x_1w)
        h_1w = h_1w[-1]  # [B, H]

        # Transformer (batch_first=True â†’ è¾“å…¥ [B, T, H])
        x_1m = self.input_projection(x_1m)            # [B, Tm, H]
        h_seq = self.transformer_1m(x_1m, x_1m)       # [B, Tm, H]
        h_1m = h_seq[:, -1, :]                        # å–æœ€åæ—¶é—´æ­¥ [B, H]

        # Fuse
        fused_trend = torch.cat([h_1d, h_1w, h_1m], dim=1)  # [B, 3H]
        fused_trend = self.feature_fusion(fused_trend)      # [B, H]

        # GRU input
        batch_size, seq_len, _ = x_1h.shape
        fused_trend_expanded = fused_trend.unsqueeze(1).repeat(1, seq_len, 1)  # [B, T, H]
        x_gru_input = torch.cat([x_1h, fused_trend_expanded], dim=2)           # [B, T, H+1]

        # GRU
        _, h_gru = self.gru(x_gru_input)
        h_gru = h_gru[-1]   # [B, H]

        # Output
        output = self.fc(h_gru)  # [B, 1]
        return output


# ============== ç®¡ç†å™¨ï¼šæ–°å¢â€œå¢é‡è®­ç»ƒâ€èƒ½åŠ› ==============

@dataclass
class ManagerConfig:
    hidden_size: int = 64
    epochs_full: int = 50
    epochs_incremental: int = 1     # æ¯å°æ—¶å¾®è°ƒçš„ epoch æ•°ï¼ˆå»ºè®® 1~2ï¼‰
    patience_full: int = 5
    lr_full: float = 1e-3
    lr_incremental: float = 2e-4
    weight_decay: float = 1e-4
    grad_clip: float = 2.0
    sequence_length: int = 24 * 30  # 720h
    forecast_length: int = 1


class MultiScaleModelManager:
    """
    - ç¬¬ä¸€æ¬¡ï¼šå®Œæ•´è®­ç»ƒåˆ° target-1hï¼Œå¹¶ä¿å­˜ meta(last_trained_until)
    - åç»­ï¼šä»…ç”¨ (last_trained_until, target] çš„æ–°æ ‡ç­¾å°æ—¶åšå¾®è°ƒï¼ˆå¢é‡è®­ç»ƒï¼‰
    """
    def __init__(self, checkpoint_dir: str = "checkpoints_multiscale", cfg: Optional[ManagerConfig] = None):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.cfg = cfg or ManagerConfig()
        self.durations = {'1h': 1, '1d': 24, '1w': 24 * 7, '1m': 24 * 30}

    # ---------- è·¯å¾„ ----------
    def _model_path(self, zone_id: int) -> Path:
        return self.checkpoint_dir / f"multiscale_zone_{zone_id}.pt"

    def _scaler_path(self, zone_id: int) -> Path:
        return self.checkpoint_dir / f"scaler_zone_{zone_id}.pkl"

    def _meta_path(self, zone_id: int) -> Path:
        return self.checkpoint_dir / f"meta_zone_{zone_id}.json"

    # ---------- çŠ¶æ€ ----------
    def has_checkpoint(self, zone_id: int) -> bool:
        return self._model_path(zone_id).exists() and self._scaler_path(zone_id).exists()

    def _save(self, zone_id: int, model: nn.Module, scaler: MinMaxScaler) -> None:
        torch.save(model.state_dict(), self._model_path(zone_id))
        import pickle
        with open(self._scaler_path(zone_id), 'wb') as f:
            pickle.dump(scaler, f)

    def _load(self, zone_id: int) -> Tuple[nn.Module, MinMaxScaler]:
        model = MultiScaleModel(self.cfg.hidden_size).to(device)
        state = torch.load(self._model_path(zone_id), map_location=device)
        model.load_state_dict(state)
        model.eval()
        import pickle
        with open(self._scaler_path(zone_id), 'rb') as f:
            scaler = pickle.load(f)
        return model, scaler

    def _save_meta(self, zone_id: int, last_trained_until: pd.Timestamp):
        meta = {"last_trained_until": str(last_trained_until)}
        with open(self._meta_path(zone_id), "w", encoding="utf-8") as f:
            json.dump(meta, f)

    def _load_meta(self, zone_id: int) -> Optional[pd.Timestamp]:
        p = self._meta_path(zone_id)
        if not p.exists():
            return None
        meta = json.load(open(p, "r", encoding="utf-8"))
        return pd.Timestamp(meta["last_trained_until"])

    # ---------- æ•°æ®å‡†å¤‡ ----------
    def _prepare_zone_series(self, df: pd.DataFrame, zone_id: int, end_inclusive: pd.Timestamp) -> pd.DataFrame:
        """è¿”å›è¯¥åŒºä»æœ€æ—©æœ‰æ•°æ®åˆ° end_inclusive çš„å®Œæ•´é€å°æ—¶åºåˆ—ï¼ˆç¼ºå¤±è¡¥ 0ï¼‰"""
        assert {'datetime', 'PULocationID'}.issubset(df.columns)
        zone_df = df[df['PULocationID'] == zone_id].copy()
        hourly = zone_df.groupby('datetime').size().reset_index(name='passenger_count')

        rng = pd.date_range(start=hourly['datetime'].min(), end=end_inclusive, freq='H') \
              if not hourly.empty else pd.date_range(end=end_inclusive, periods=self.cfg.sequence_length + 1, freq='H')
        hourly = (
            hourly.set_index('datetime')
                  .reindex(rng)
                  .fillna(0.0)
                  .rename_axis('datetime')
                  .reset_index()
        )
        hourly['passenger_count'] = hourly['passenger_count'].astype(float)
        return hourly  # åˆ—: datetime, passenger_count

    def _fit_scaler_hist(self, hourly: pd.DataFrame, fit_until_exclusive: pd.Timestamp) -> MinMaxScaler:
        """åªç”¨ < fit_until_exclusive çš„å†å²æ‹Ÿåˆ scalerï¼Œé¿å…æœªæ¥æ³„æ¼"""
        scaler = MinMaxScaler()
        hist = hourly[hourly['datetime'] < fit_until_exclusive]
        if hist.empty:
            # è‹¥å†å²ä¸ºç©ºï¼Œé€€åŒ–ä¸ºå…¨é‡ï¼ˆé¿å…æŠ¥é”™ï¼‰ï¼›å®é™…åœºæ™¯å¯è‡ªå®šä¹‰
            hist = hourly
        scaler.fit(hist[['passenger_count']])
        return scaler

    def _build_windows(self, series_scaled: pd.Series) -> Tuple[dict, torch.Tensor]:
        """ä»ä¸€æ®µè¿ç»­çš„ç¼©æ”¾ååºåˆ—æ„é€ å¤šæ ·æœ¬æ»‘çª—ï¼ˆæœ€åä¸€æ­¥ä¸ºæ ‡ç­¾ï¼‰"""
        L = self.cfg.sequence_length
        flen = self.cfg.forecast_length
        dur = self.durations

        X_1h, X_1d, X_1w, X_1m, Y = [], [], [], [], []
        n = len(series_scaled)
        for i in range(n + 1 - L - flen):
            x_1h = series_scaled.iloc[i + L - dur['1h']: i + L].values
            x_1d = series_scaled.iloc[i + L - dur['1d']: i + L].values
            x_1w = series_scaled.iloc[i + L - dur['1w']: i + L].values
            x_1m = series_scaled.iloc[i: i + L].values
            y = series_scaled.iloc[i + L: i + L + flen].values  # flen=1

            X_1h.append(x_1h)
            X_1d.append(x_1d)
            X_1w.append(x_1w)
            X_1m.append(x_1m)
            Y.append(y)

        X = {
            '1h': torch.tensor(np.array(X_1h), dtype=torch.float32).unsqueeze(-1).to(device),
            '1d': torch.tensor(np.array(X_1d), dtype=torch.float32).unsqueeze(-1).to(device),
            '1w': torch.tensor(np.array(X_1w), dtype=torch.float32).unsqueeze(-1).to(device),
            '1m': torch.tensor(np.array(X_1m), dtype=torch.float32).unsqueeze(-1).to(device),
        }
        Y = torch.tensor(np.array(Y), dtype=torch.float32).to(device)
        return X, Y

    def _build_inference_window(self, hourly: pd.DataFrame, scaler: MinMaxScaler, target_date: pd.Timestamp) -> dict:
        """
        ç”¨ [target-720, target-1] çš„ 720 ä¸ªç‚¹åšè¾“å…¥ï¼›
        æ³¨æ„ï¼šä»… transformï¼Œä¸é‡æ–° fitï¼Œé¿å…æ³„æ¼
        """
        dur = self.durations
        L = self.cfg.sequence_length

        df = hourly.copy()
        df['passenger_count_scaled'] = scaler.transform(df[['passenger_count']])

        # æ‰¾åˆ° target çš„è¡Œå·
        idx = {t: i for i, t in enumerate(df['datetime'])}
        if target_date not in idx:
            raise ValueError("target_date ä¸åœ¨è¯¥åŒºé—´åºåˆ—å†…ï¼ˆè¯·æ£€æŸ¥ DF æ˜¯å¦è¦†ç›– targetï¼‰")
        end_i = idx[target_date]           # æ ‡ç­¾åœ¨ target_date
        start_i = end_i - L
        if start_i < 0:
            raise ValueError("å†å²ä¸è¶³ 720 å°æ—¶ï¼Œæ— æ³•æ„é€ æ¨ç†çª—å£")

        series = df['passenger_count_scaled']
        x_1h = series.iloc[end_i - dur['1h']: end_i].values
        x_1d = series.iloc[end_i - dur['1d']: end_i].values
        x_1w = series.iloc[end_i - dur['1w']: end_i].values
        x_1m = series.iloc[start_i: end_i].values

        X = {
            '1h': torch.tensor(x_1h, dtype=torch.float32, device=device).view(1, -1, 1),
            '1d': torch.tensor(x_1d, dtype=torch.float32, device=device).view(1, -1, 1),
            '1w': torch.tensor(x_1w, dtype=torch.float32, device=device).view(1, -1, 1),
            '1m': torch.tensor(x_1m, dtype=torch.float32, device=device).view(1, -1, 1),
        }
        return X

    # ---------- è®­ç»ƒ/é¢„æµ‹ ----------
    def train_once(self, df: pd.DataFrame, zone_id: int, target_date: pd.Timestamp) -> None:
        """ç¬¬ä¸€æ¬¡ï¼šç”¨æ‰€æœ‰ < target çš„å†å²æ„é€ å¤šæ ·æœ¬æ»‘çª—ï¼Œå®Œæ•´è®­ç»ƒï¼Œç„¶åæŠŠ meta è®°åˆ° target-1h"""
        hourly = self._prepare_zone_series(df, zone_id, end_inclusive=target_date)
        scaler = self._fit_scaler_hist(hourly, fit_until_exclusive=target_date)

        # ä»…ç”¨ < target çš„æ•°æ®æ„çª—ï¼ˆæœ€åæ ‡ç­¾ â‰¤ target-1hï¼‰
        hourly_hist = hourly[hourly['datetime'] < target_date].reset_index(drop=True)
        series_scaled = pd.Series(
            scaler.transform(hourly_hist[['passenger_count']]).squeeze(),
            index=hourly_hist.index
        )
        X, Y = self._build_windows(series_scaled)

        model = MultiScaleModel(self.cfg.hidden_size).to(device)
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=self.cfg.lr_full, weight_decay=self.cfg.weight_decay)

        best = float('inf'); patience = 0
        model.train()
        for ep in range(self.cfg.epochs_full):
            optimizer.zero_grad()
            pred = model(X)
            loss = criterion(pred, Y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), self.cfg.grad_clip)
            optimizer.step()

            cur = loss.item()
            if cur < best:
                best, patience = cur, 0
            else:
                patience += 1
                if patience >= self.cfg.patience_full:
                    break

        self._save(zone_id, model, scaler)
        self._save_meta(zone_id, last_trained_until=target_date - pd.Timedelta(hours=1))

    def incremental_update(
        self,
        df: pd.DataFrame,
        zone_id: int,
        prev_until: pd.Timestamp,
        new_until: pd.Timestamp,
        epochs: Optional[int] = None,
        lr: Optional[float] = None,
    ) -> None:
        """
        ç”¨ (prev_until, new_until] çš„æ¯ä¸ªæ ‡ç­¾å°æ—¶ s åšå¾®è°ƒã€‚
        å¯¹äºæ¯ä¸ª sï¼šè¾“å…¥çª—å£ = [s-720, s-1]ï¼Œæ ‡ç­¾ = sã€‚
        """
        if new_until <= prev_until:
            return

        epochs = epochs if epochs is not None else self.cfg.epochs_incremental
        lr = lr if lr is not None else self.cfg.lr_incremental

        model, scaler = self._load(zone_id)
        model.train()
        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=self.cfg.weight_decay)
        criterion = nn.SmoothL1Loss()  # ç¨ç¨³å¥ä¸€äº›

        # å‡†å¤‡å®Œæ•´åºåˆ—ï¼ˆè¦†ç›–åˆ° new_untilï¼‰å¹¶ä»… transform
        hourly = self._prepare_zone_series(df, zone_id, end_inclusive=new_until)
        hourly['passenger_count_scaled'] = scaler.transform(hourly[['passenger_count']])
        idx = {t: i for i, t in enumerate(hourly['datetime'])}

        s_list = pd.date_range(start=prev_until + pd.Timedelta(hours=1), end=new_until, freq="H")
        dur = self.durations
        L = self.cfg.sequence_length
        series = hourly['passenger_count_scaled']

        X_1h, X_1d, X_1w, X_1m, Y = [], [], [], [], []
        for s in s_list:
            if s not in idx:
                continue
            end_i = idx[s]           # æ ‡ç­¾åœ¨ s
            start_i = end_i - L
            if start_i < 0:
                continue
            if end_i + self.cfg.forecast_length > len(series):
                continue

            X_1h.append(series.iloc[end_i - dur['1h']: end_i].values)
            X_1d.append(series.iloc[end_i - dur['1d']: end_i].values)
            X_1w.append(series.iloc[end_i - dur['1w']: end_i].values)
            X_1m.append(series.iloc[start_i: end_i].values)
            Y.append(series.iloc[end_i: end_i + self.cfg.forecast_length].values)

        if not X_1m:
            # æ²¡æœ‰æ–°å¢æ ·æœ¬ï¼ˆä¾‹å¦‚æ•°æ®ç¼ºå¤±ï¼‰
            self._save(zone_id, model, scaler)
            self._save_meta(zone_id, prev_until)
            return

        X = {
            '1h': torch.tensor(np.array(X_1h), dtype=torch.float32, device=device).unsqueeze(-1),
            '1d': torch.tensor(np.array(X_1d), dtype=torch.float32, device=device).unsqueeze(-1),
            '1w': torch.tensor(np.array(X_1w), dtype=torch.float32, device=device).unsqueeze(-1),
            '1m': torch.tensor(np.array(X_1m), dtype=torch.float32, device=device).unsqueeze(-1),
        }
        Y = torch.tensor(np.array(Y), dtype=torch.float32, device=device)

        for _ in range(max(1, epochs)):
            optimizer.zero_grad()
            pred = model(X)
            loss = criterion(pred, Y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), self.cfg.grad_clip)
            optimizer.step()

        self._save(zone_id, model, scaler)
        self._save_meta(zone_id, new_until)

    def predict(self, df: pd.DataFrame, zone_id: int, target_date: pd.Timestamp) -> float:
        """ç”¨ä¿å­˜çš„æ¨¡å‹å¯¹ target_date å°æ—¶åšé¢„æµ‹ï¼ˆè¾“å…¥ä¸º [target-720, target-1]ï¼‰"""
        if not self.has_checkpoint(zone_id):
            raise FileNotFoundError(f"Zone {zone_id} æ²¡æœ‰ checkpointï¼Œè¯·å…ˆè®­ç»ƒ")

        model, scaler = self._load(zone_id)
        hourly = self._prepare_zone_series(df, zone_id, end_inclusive=target_date)
        X_last = self._build_inference_window(hourly, scaler, target_date)

        model.eval()
        with torch.no_grad():
            pred_scaled = model(X_last).cpu().numpy()  # shape (1,1)
        pred = scaler.inverse_transform(pred_scaled)[0, 0]
        return float(pred)

    def train_and_predict_if_needed(self, df: pd.DataFrame, zone_id: int, target_date: pd.Timestamp,
                                    auto_train: bool = True) -> float:
        """
        ç»Ÿä¸€å…¥å£ï¼š
        - è‹¥æ—  ckpt â†’ å®Œæ•´è®­ç»ƒåˆ° target-1h
        - è‹¥æœ‰ ckpt â†’ å¯¹ (last_trained_until, target] åšå¢é‡å¾®è°ƒ
        - ç„¶åé¢„æµ‹ target
        """
        if not self.has_checkpoint(zone_id):
            if not auto_train:
                raise FileNotFoundError(f"No checkpoint for zone {zone_id} and auto_train is False.")
            self.train_once(df, zone_id, target_date)
        else:
            prev = self._load_meta(zone_id)
            if prev is None:
                # æ—§æ¨¡å‹æ²¡æœ‰ metaï¼Œä¿å®ˆèµ·è§å®Œæ•´è®­ä¸€æ¬¡
                self.train_once(df, zone_id, target_date)
            elif prev < target_date:
                self.incremental_update(df, zone_id, prev_until=prev, new_until=target_date)
        return self.predict(df, zone_id, target_date)


# ============== ä¸€ä¸ªå¯ç›´æ¥è¿è¡Œçš„ main()ï¼ˆIDE é‡Œç‚¹ â–¶ï¸ å³å¯ï¼‰ ==============

# ç”¨æˆ·å¯æ”¹çš„é…ç½®
DATA_PATH = "data.parquet"
LOOKUP_PATH = "taxi-zone-lookup.csv"
CHECKPOINT_DIR = "checkpoints_multiscale_inc"

START_TARGET = pd.Timestamp("2021-03-05 12:00")  # ç¬¬ 721 å°æ—¶
ROLLING_STEPS = 2                                 # è¿ç»­é¢„æµ‹å°æ—¶æ•°
EXCLUDED_ZONES = [103, 104, 105, 46, 264, 265]    # è¿‡æ»¤çš„åŒºåŸŸ
RETRAIN_EACH_HOUR = False                          # True = æ¯å°æ—¶ä»é›¶é‡è®­ï¼ˆä»…è°ƒè¯•ç”¨ï¼‰


def _prepare_df() -> pd.DataFrame:
    df = pd.read_parquet(DATA_PATH, columns=["pickup_datetime", "PULocationID"])
    df["pickup_datetime"] = pd.to_datetime(df["pickup_datetime"])
    df["datetime"] = df["pickup_datetime"].dt.floor("H")
    if EXCLUDED_ZONES:
        df = df[~df["PULocationID"].isin(EXCLUDED_ZONES)].copy()
    return df


def _get_true_counts(df: pd.DataFrame, hour: pd.Timestamp) -> pd.Series:
    return df[df["datetime"] == hour].groupby("PULocationID").size()


def run_rolling(df: pd.DataFrame):
    zones = sorted(df["PULocationID"].unique().tolist())
    cfg = ManagerConfig()
    base_mgr = MultiScaleModelManager(checkpoint_dir=CHECKPOINT_DIR, cfg=cfg)

    rows, hours = [], []
    for i in range(ROLLING_STEPS):
        target = START_TARGET + pd.Timedelta(hours=i)
        print(f"\nğŸ•’ Target hour: {target}")

        # å¯é€‰ï¼šæ¯å°æ—¶é‡è®­ï¼ˆç‹¬ç«‹ç›®å½•ï¼Œéå¢é‡ï¼Œä»…ä¾›å¯¹ç…§/è°ƒè¯•ï¼‰
        mgr = base_mgr
        if RETRAIN_EACH_HOUR:
            mgr = MultiScaleModelManager(
                checkpoint_dir=str(Path(CHECKPOINT_DIR) / target.strftime("%Y%m%d_%H%M")),
                cfg=cfg
            )

        y_true_map = _get_true_counts(df, target)
        preds, trues = [], []
        for zid in zones:
            try:
                y_pred = mgr.train_and_predict_if_needed(df, zid, target, auto_train=True)
                y_true = float(y_true_map.get(zid, 0.0))
                rows.append({"target_hour": target, "PULocationID": zid, "y_pred": y_pred, "y_true": y_true})
                preds.append(y_pred); trues.append(y_true)
            except Exception as e:
                rows.append({"target_hour": target, "PULocationID": zid, "y_pred": np.nan, "y_true": np.nan, "error": str(e)})

        # å°æ—¶çº§æŒ‡æ ‡
        preds = np.array(preds, dtype=float); trues = np.array(trues, dtype=float)
        m = ~np.isnan(preds) & ~np.isnan(trues)
        if m.any():
            mae = float(np.mean(np.abs(preds[m] - trues[m])))
            rmse = float(np.sqrt(np.mean((preds[m] - trues[m]) ** 2)))
            hours.append({"target_hour": target, "MAE": mae, "RMSE": rmse, "N": int(m.sum())})
            print(f"âœ… Hourly MAE={mae:.3f}, RMSE={rmse:.3f}")
        else:
            hours.append({"target_hour": target, "MAE": np.nan, "RMSE": np.nan, "N": 0})
            print("âš ï¸ æ— æœ‰æ•ˆæ ·æœ¬è®¡ç®—æŒ‡æ ‡")

    # ä¿å­˜
    pred_df = pd.DataFrame(rows)
    pred_df.to_csv("predictions_rolling.csv", index=False)
    hour_df = pd.DataFrame(hours).sort_values("target_hour")
    hour_df.to_csv("hourly_metrics.csv", index=False)

    # Overall
    m = pred_df.dropna(subset=["y_pred", "y_true"])
    if not m.empty:
        overall_mae = float(np.mean(np.abs(m["y_pred"].values - m["y_true"].values)))
        overall_rmse = float(np.sqrt(np.mean((m["y_pred"].values - m["y_true"].values) ** 2)))
    else:
        overall_mae = overall_rmse = np.nan
    with open("overall_metrics.txt", "w", encoding="utf-8") as f:
        f.write(f"Overall MAE: {overall_mae}\nOverall RMSE: {overall_rmse}\n")
    print(f"\nğŸ¯ Overall MAE={overall_mae:.4f}, RMSE={overall_rmse:.4f}")
    print("å·²ä¿å­˜ï¼špredictions_rolling.csv / hourly_metrics.csv / overall_metrics.txt")


def main():
    df = _prepare_df()
    run_rolling(df)


if __name__ == "__main__":
    main()
